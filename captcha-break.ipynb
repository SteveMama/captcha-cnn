{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/BenjaminWegener/CaptchaSolver\n",
    "def resize_to_fit(image, width, height):\n",
    "    \"\"\"\n",
    "    A helper function to resize an image to fit within a given size\n",
    "    :param image: image to resize\n",
    "    :param width: desired width in pixels\n",
    "    :param height: desired height in pixels\n",
    "    :return: the resized image\n",
    "    \"\"\"\n",
    "\n",
    "    # grab the dimensions of the image, then initialize\n",
    "    # the padding values\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if the width is greater than the height then resize along\n",
    "    # the width\n",
    "    if w > h:\n",
    "        image = imutils.resize(image, width=width)\n",
    "\n",
    "    # otherwise, the height is greater than the width so resize\n",
    "    # along the height\n",
    "    else:\n",
    "        image = imutils.resize(image, height=height)\n",
    "\n",
    "    # determine the padding values for the width and height to\n",
    "    # obtain the target dimensions\n",
    "    padW = int((width - image.shape[1]) / 2.0)\n",
    "    padH = int((height - image.shape[0]) / 2.0)\n",
    "\n",
    "    # pad the image then apply one more resizing to handle any\n",
    "    # rounding issues\n",
    "    image = cv2.copyMakeBorder(image, padH, padH, padW, padW,\n",
    "        cv2.BORDER_REPLICATE)\n",
    "    image = cv2.resize(image, (width, height))\n",
    "\n",
    "    # return the pre-processed image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento do captcha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualidade da imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImage(img):    \n",
    "    imgBorder = cv2.copyMakeBorder(img, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    ret, imgThreshold = cv2.threshold(imgBorder, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#     plt.imshow(imgThreshold, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    kernel = np.ones((1,2), np.uint8)\n",
    "    imgErode = cv2.erode(imgThreshold, kernel, iterations = 1)\n",
    "\n",
    "#     plt.imshow(imgErode, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    return imgErode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"tests/01.png\", 0)\n",
    "\n",
    "imgErode = processImage(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLetters(imgErode):\n",
    "    contours = cv2.findContours(imgErode, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    contours = contours[0]\n",
    "    imageRegions = []\n",
    "    areaArr = []\n",
    "\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        areaArr.append(area)\n",
    "    \n",
    "    # encontrando mediana para descartar áreas fora do padrão\n",
    "    median = 0\n",
    "    areaArr.sort()\n",
    "    if len(areaArr)%2 != 0:\n",
    "        mid = int(len(areaArr)/2)\n",
    "        median = areaArr[mid]\n",
    "    else:\n",
    "        mid = int(len(areaArr)/2)\n",
    "        median = (areaArr[mid] + areaArr[mid - 1])/2\n",
    "    \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 3*median or area < median/3:\n",
    "            continue;\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        imgRGB = cv2.cvtColor(imgErode,cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "        # se duas letras estiverem juntas\n",
    "        if w/h > 1.25:\n",
    "            half_width = int(w/2)\n",
    "            imageRegions.append((x, y, half_width, h))\n",
    "            imageRegions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            imageRegions.append((x, y, w, h))\n",
    "    \n",
    "    imageRegions = sorted(imageRegions, key=lambda x: x[0])\n",
    "    \n",
    "    return imageRegions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRegions(imageRegions):\n",
    "    for point in imageRegions:    \n",
    "        imgRGB = cv2.cvtColor(imgErode, cv2.COLOR_GRAY2RGB)\n",
    "        imgRect = cv2.rectangle(imgRGB, (point[0], point[1]), (point[0]+point[2], point[1]+point[3]), (255,0,0), 1)\n",
    "\n",
    "        plt.imshow(imgRect)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC5CAYAAAAxiWT3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA/RJREFUeJzt3dFu2kAARcG6yv//Mn2taEqysdfeg2ce2yoQQEebGxe2x+PxC4Ce31ffAQB+RsABogQcIErAAaIEHCBKwAGiBBwgSsABoj5Ovj3/awhg3PbZHzqBA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThA1MfVdwBY07Zt3/63j8dj4j3hf5zAAaIEHCBKwAGibOB7DeyEKTbNtzeycbMmJ3CAKAEHiBJwgCgb+NEGt+NZO+Twdbn20Nt5fo2MvhZd+309J3CAKAEHiBJwgCgb+MH2bNp7N0mOc+Vjb1vmu5zAAaIEHCDKhHKyVz8e7/2xfZUfvY+cH676nmZeYvfV13r++1WeV9bjBA4QJeAAUQIOEGUD53B79uORvferr/vV13KZ5j5/P352+ms4gQNECThAlIADRNnAD3bmFnjH3XHWNeYr7eF3fF75GSdwgCgBB4gScIAoG/jFVtpe382V7ynieeUMTuAAUQIOECXgAFE2cJY2c0tedaf2fuB8lxM4QJSAA0SZUE525KfWM8/Mj1R7NvIRa14D/M0JHCBKwAGiBBwgygbOUkY+Jm10l171skH4KSdwgCgBB4gScIAoG/jCRq/5fbXxun74tSsfn73XnHNfTuAAUQIOECXgAFE28MlW2TOrb1G6yv2cuVOv8hoZtcpzc2dO4ABRAg4QJeAAUTbwhR25jdor/zXzMXn1Ht4z31uce3ECB4gScIAoAQeIsoEv7I7b56rf857fR1SvwX9Wvd/vzAkcIErAAaJMKJP5sfO1PY/P6OV5R95WxZ7pp/o934kTOECUgANECThAlA38aNG3Bp3Jlnodj/17cwIHiBJwgCgBB4jaTt7IDHIA4z795ZoTOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThA1MfJt7edfHsAb8sJHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIOoPvlamf/d9UAIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC5CAYAAAAxiWT3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA/FJREFUeJzt3dlyGjEQQNGQ4v9/mby6cIw9nkW66JzHLAYG6lbTUeD2eDz+ANDzd/QdAOB3BBwgSsABogQcIErAAaIEHCBKwAGiBBwg6n7x7flfQwDb3f73iyZwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIuo++A8Ccbrfbj//s4/E48Z7wFRM4QJSAA0QJOECUHTgsasuOmzmZwAGiBBwgSsABouzABztrD3npudx32KUueI75+TWy9bXo7Pd4JnCAKAEHiBJwgCg78IPt2Wnv3UlOo7IbfXF9R157u2V+ygQOECXgAFFWKBd79fZ479v2Wd56H7l+GPWYzjxi993Pev79WZ5X5mMCB4gScIAoAQeIsgPncHv2x1v2vd/93CN/Fp99vGb29GOYwAGiBBwgSsABouzAD3blLnDFveMKu+oVn1d+xwQOECXgAFECDhBlBz7YCjvdUUZ+pojnlSuYwAGiBBwgSsABouzAmdqKu2SfB85PmcABogQcIMoK5WJHfms95znzK9WebfmKNa8BPjKBA0QJOECUgANE2YEzlS1fk7Z1L/3peN6mvw3zMYEDRAk4QJSAA0TZgU9s65nfVzth54dfG3l99p45Z10mcIAoAQeIEnCAKDvwk82yz6x+ROks9/PMPfUsr5GtZnluVmYCB4gScIAoAQeIsgOf2JG7UfvKa736DO8zP1uctZjAAaIEHCBKwAGi7MAntuLuc9bHvOffI6pn8J9V7/c7M4EDRAk4QJQVysm87Xxtz/XZejzv021tWItUn8c9q5/qY16JCRwgSsABogQcIMoOnNPZpY7j2r83EzhAlIADRAk4QJQdOG/DvpfVmMABogQcIErAAaLswDnegV8FB3zNBA4QJeAAUQIOEGUHzn7OX8MQJnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAqPvFt3e7+PYA3pYJHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIOofV1WiheE1aq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC5CAYAAAAxiWT3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA+9JREFUeJzt3Vty2kAARUGUYv9bVn4xJNiyXnNQ92ceBgN1anyjwDTP8w2Anj9n3wEAfkfAAaIEHCBKwAGiBBwgSsABogQcIErAAaLuB9+e/zUEsNz0r190AgeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgKj72XcAGNM0TT/+s/M873hP+B8ncIAoAQeIEnCAKBs4XNSSjZsxOYEDRAk4QJSAA0TZwE+21w7putwf+IQNeMXz/PwaWfpa9Bo7nxM4QJSAA0QJOECUDXxjazbttZskKwz02L/dlr0meOAEDhAl4ABRJpSDvfvxeO2P7aNc1rXl/HDW97TnJXbffa3n3x/leWU8TuAAUQIOECXgAFE2cDa3Zj9esvd+93W3/Fq8enzM7PTncAIHiBJwgCgBB4iygW/syC3wirvjFbbqKz6v/I4TOECUgANECThAlA38ZFfYdM9y5nuKeF45ghM4QJSAA0QJOECUDZyhXXFL9n7g/JQTOECUgANEmVAOtuWn1rOfPT9S7dmSj1jzCuCREzhAlIADRAk4QJQNnKEs+Zi0pbv0y+V5i/42jMcJHCBKwAGiBBwgygY+sKXXfb/bhF1D/t6Zj8/aa865LidwgCgBB4gScIAoG/jORtkzq29ROsr93HOnHuU1stQoz82VOYEDRAk4QJSAA0TZwAe25TZqrzzWl/fw3vK9xaN7OftwAgeIEnCAKAEHiLKBD+yKu/Wo3/Oaf4+oXoP/rHq/P5kTOECUgANEmVB25sfO99Y8Pksvz3u5rQWzSPV5XDP9VL/nK3ECB4gScIAoAQeIsoGzO1vqeTz2n80JHCBKwAGiBBwgygbOx7D3cjVO4ABRAg4QJeAAUTZwuN18VBlJTuAAUQIOECXgAFE2cK7LdePEOYEDRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRN0Pvr3p4NsD+FhO4ABRAg4QJeAAUQIOECXgAFECDhAl4ABRAg4QJeAAUQIOECXgAFECDhAl4ABRAg4QJeAAUQIOECXgAFECDhAl4ABRAg4QJeAAUQIOECXgAFF/ATSWoIme0ff3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC5CAYAAAAxiWT3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA/hJREFUeJzt3dtu2kAARdFS8f+/TF4jgiCOLzPbXuuxlQIBtDWcunB7PB7/AOj5P/oOAPA3Ag4QJeAAUQIOECXgAFECDhAl4ABRAg4QdT/49vyvIYDlbq/+0AkcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABou6j7wCwodtt9D147fEYfQ9OyQkcIErAAaIEHCDKBg5ntmJ7vi3Y0x/PtzPrFn8yTuAAUQIOECXgAFE2cLioJRs3c3ICB4gScIAoAQeIsoEPttcO+eO6XHjy/BpZ+lr0GhvPCRwgSsABogQcIMoGvrE1m/baTZLtjHzsbcv8lhM4QJSAA0SZUA727u3x2rfts7z13nJ+GPU77XmJ3aef9fz3szyvzMcJHCBKwAGiBBwgygbO5tbsx0v23k8/99PPcpnmOt8fPzv9GE7gAFECDhAl4ABRNvCNHbkFXnF33Osa85n28Cs+r/yNEzhAlIADRAk4QJQNfLCZttezGfmZIp5XjuAEDhAl4ABRAg4QZQNnantuybPu1D4PnN9yAgeIEnCAKBPKwbb81nr2s+dXqj1b8hVrXgN85wQOECXgAFECDhBlA2cqS74mbekuPetlg/BXTuAAUQIOECXgAFE28Iktveb33cbr+uH3Rj4+a68557qcwAGiBBwgSsABomzgO5tlz6x+ROks93PPnXqW18hSszw3V+YEDhAl4ABRAg4QZQOf2JbbqL3ypz0fk3ef4b3nZ4tzLU7gAFECDhAl4ABRNvCJXXH7nPV3XvPvEdVr8J9V7/eZOYEDRAk4QJQJZWfedr635vFZennelrdVsWb6qf7OV+IEDhAl4ABRAg4QZQNnd7bUcTz25+YEDhAl4ABRAg4QZQPnNOy9L0S/ro3fcQIHiBJwgCgBB4iygcOZ+HeAS3ECB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiLoffHu3g28P4LScwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaK+ADw8p4Qld+EmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageRegions = findLetters(imgErode)\n",
    "printRegions(imageRegions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando processamento nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish processing\n"
     ]
    }
   ],
   "source": [
    "FOLDER_INPUT = \"data\"\n",
    "FOLDER_OUTPUT = \"data_letters\"\n",
    "captchaImgs = glob.glob(os.path.join(FOLDER_INPUT, \"*\"))\n",
    "countLetter = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "countNumber = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for i, file in enumerate(captchaImgs):\n",
    "    name = os.path.basename(file)\n",
    "    captchaText = os.path.splitext(name)[0] # pegando só texto (sem \".png\")\n",
    "    pathImg = \"data/\" + name\n",
    "#    print(\"Processando\", i, \"de\", len(captchaImgs), \"-\", pathImg)\n",
    "    \n",
    "    img = cv2.imread(pathImg, 0)\n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "    imgErode = processImage(img)\n",
    "    \n",
    "    imageRegions = findLetters(imgErode)\n",
    "#     printRegions(imageRegions)\n",
    "    if len(imageRegions) != 4:\n",
    "        continue;\n",
    "        \n",
    "    for points, letter in zip(imageRegions, captchaText):\n",
    "        x, y, w, h = points\n",
    "        \n",
    "        imageSave = imgErode[y:y + h, x:x + w]\n",
    "        \n",
    "#         plt.imshow(imageSave)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "        \n",
    "        pathSave = os.path.join(FOLDER_OUTPUT, letter)\n",
    "    \n",
    "        if not os.path.exists(pathSave):\n",
    "            os.makedirs(pathSave)\n",
    "        \n",
    "        ans = ord(letter)\n",
    "\n",
    "        if ans >= 0 and ans < ord('A'):\n",
    "            idx = ans - ord('0')\n",
    "            countNumber[idx] = countNumber[idx] + 1\n",
    "            strImgPath = str(countNumber[idx]) + \".png\"\n",
    "        else:\n",
    "            idx = ans - ord('A')\n",
    "            countLetter[idx] = countLetter[idx] + 1\n",
    "            strImgPath = str(countLetter[idx]) + \".png\"\n",
    "        \n",
    "#         print(strImgPath)\n",
    "        pt = os.path.join(pathSave, strImgPath)\n",
    "        cv2.imwrite(pt, imageSave)\n",
    "\n",
    "print(\"finish processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish append\n"
     ]
    }
   ],
   "source": [
    "FOLDER_LETTERS_INPUT = \"data_letters\"\n",
    "\n",
    "dataImages = []\n",
    "dataLabels = []\n",
    "\n",
    "for file in paths.list_images(FOLDER_LETTERS_INPUT):\n",
    "    image = cv2.imread(file, 0)\n",
    "    image = resize_to_fit(image, 20, 20)\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    label = file.split(os.path.sep)[-2]\n",
    "    \n",
    "#    print(\"append data...\")\n",
    "    dataImages.append(image)\n",
    "    dataLabels.append(label)\n",
    "\n",
    "print(\"finish append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34832\n",
      "34832\n"
     ]
    }
   ],
   "source": [
    "print(len(dataImages))\n",
    "print(len(dataLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LABELS_MODEL = \"label_model.dat\"\n",
    "\n",
    "dataImages = np.array(dataImages)/255\n",
    "dataLabels = np.array(dataLabels)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "dataLabels = encoder.fit_transform(dataLabels)\n",
    "\n",
    "(Xtrain, Xtest, Ytrain, Ytest) = train_test_split(dataImages, dataLabels, test_size=0.25, random_state=70)\n",
    "\n",
    "with open(FILE_LABELS_MODEL, \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/izabellamelo/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 20)        100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 64)        5184      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 128)         32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 256)         131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               128500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                16032     \n",
      "=================================================================\n",
      "Total params: 314,040\n",
      "Trainable params: 314,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = \"model.hdf5\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(20, (2, 2), padding=\"same\", input_shape=(20, 20, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (2, 2), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (2, 2), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (2, 2), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(32, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adagrad\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/izabellamelo/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 26124 samples, validate on 8708 samples\n",
      "Epoch 1/10\n",
      "26124/26124 [==============================] - 10s 387us/step - loss: 0.4556 - acc: 0.8725 - val_loss: 0.0642 - val_acc: 0.9892\n",
      "Epoch 2/10\n",
      "26124/26124 [==============================] - 10s 367us/step - loss: 0.0478 - acc: 0.9909 - val_loss: 0.0412 - val_acc: 0.9929\n",
      "Epoch 3/10\n",
      "26124/26124 [==============================] - 9s 341us/step - loss: 0.0312 - acc: 0.9938 - val_loss: 0.0357 - val_acc: 0.9939\n",
      "Epoch 4/10\n",
      "26124/26124 [==============================] - 9s 346us/step - loss: 0.0234 - acc: 0.9954 - val_loss: 0.0308 - val_acc: 0.9947\n",
      "Epoch 5/10\n",
      "26124/26124 [==============================] - 9s 345us/step - loss: 0.0182 - acc: 0.9966 - val_loss: 0.0298 - val_acc: 0.9948\n",
      "Epoch 6/10\n",
      "26124/26124 [==============================] - 10s 370us/step - loss: 0.0149 - acc: 0.9971 - val_loss: 0.0296 - val_acc: 0.9952\n",
      "Epoch 7/10\n",
      "26124/26124 [==============================] - 9s 352us/step - loss: 0.0132 - acc: 0.9974 - val_loss: 0.0289 - val_acc: 0.9951\n",
      "Epoch 8/10\n",
      "26124/26124 [==============================] - 10s 364us/step - loss: 0.0109 - acc: 0.9979 - val_loss: 0.0281 - val_acc: 0.9953\n",
      "Epoch 9/10\n",
      "26124/26124 [==============================] - 9s 335us/step - loss: 0.0095 - acc: 0.9984 - val_loss: 0.0283 - val_acc: 0.9954\n",
      "Epoch 10/10\n",
      "26124/26124 [==============================] - 9s 328us/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.0285 - val_acc: 0.9954\n"
     ]
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, validation_data=(Xtest, Ytest), batch_size=32, epochs=10, verbose=1)\n",
    "model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando com captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage(img):\n",
    "    dataImagesPredict = []\n",
    "\n",
    "    imgErode = processImage(img)\n",
    "    imageRegions = findLetters(imgErode)\n",
    "\n",
    "    for points in imageRegions:\n",
    "        x, y, w, h = points\n",
    "\n",
    "        image = imgErode[y:y + h, x:x + w]\n",
    "\n",
    "        image = resize_to_fit(image, 20, 20)\n",
    "        image = np.expand_dims(image, axis=2)\n",
    "\n",
    "        dataImagesPredict.append(image)\n",
    "\n",
    "    dataImagesPredict = np.array(dataImagesPredict)/255\n",
    "\n",
    "    for element in dataImagesPredict:\n",
    "        prediction = model.predict(dataImagesPredict)\n",
    "        letter = encoder.inverse_transform(prediction)\n",
    "\n",
    "    print(\"CAPTCHA TEXT:\", letter)\n",
    "    plt.imshow(imgErode, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTCHA TEXT: ['Z' 'W' 'B' 'J']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC5CAYAAAAxiWT3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA+JJREFUeJzt3dFum0AUQMFS+f9/2X2NnNQOhoU9ZuaxrWwC6Gi5obDc7/c/APT8PXsDAHiPgANECThAlIADRAk4QJSAA0QJOECUgANE3Q7+Pv9rCGC95ac/tAIHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScICo29kbAMxpWZZf/9v7/T5wS/gfK3CAKAEHiBJwgCgzcLioNTNu5mQFDhAl4ABRAg4QZQZ+slFzSPfl8srjObL2XHSOnc8KHCBKwAGiBBwgygx8Z1tm2ltnkuznzH1vtsxvWYEDRAk4QJQRysGeXR5vvWyf5dJ7z/HDWT/TyFvsXn3W49/PclyZjxU4QJSAA0QJOECUGTi72zI/XjPvffW5rz7LbZrbfN1/5vTnsAIHiBJwgCgBB4gyA9/ZkbPAK84dR91jPtM8/IrHlfdYgQNECThAlIADRJmBn2ym2eunOfOZIo4rR7ACB4gScIAoAQeIMgNnaiNnybPOqT0PnN+yAgeIEnCAKCOUg+351nrGGflKtUdrXrHmHOArK3CAKAEHiBJwgCgzcKay5jVpa+fSs942CO+yAgeIEnCAKAEHiDIDn9jae36fzXjdP/zcmftn6z3nXJcVOECUgANECThAlBn4YLPMM6uPKJ1lO0fOqWc5R9aa5dhcmRU4QJSAA0QJOECUGfjE9pyNmld+N3KfPHuG98hni3MtVuAAUQIOECXgAFFm4BO74uxz1p95y+8jqvfgP6pu9yezAgeIEnCAqOXgyyLXYBd01GNu17yO7Sq2jH6uuL8m9uOBtAIHiBJwgCgBB4hyGyHDmaWex77/bFbgAFECDhAl4ABRZuB8DPNersYKHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwg6nbw9y0Hfx/Ax7ICB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiPoHzGichkTWA/QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTCHA TEXT: ['C' 'G' 'L' 'T']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAC5CAYAAAAxiWT3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA7FJREFUeJzt3dtO6lAUQFF6wv//cn08ShQt9LJnO8ajCfQCzmwXrUzzPN8A6Pl39A4A8BoBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4i677w9dw0BLDd990MrcIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIEnCAKAEHiBJwgCgBB4gScIAoAQeIuh+9A3AG0zS9/Nh5nlfcE67EChwgSsABogQcIMoMPMzcdRyP5/Od1wb+ygocIErAAaIEHCDKDHxgS+eoz+baj8/123ObkW+nem73mutXz88RrMABogQcIMoIZSBrjkxGteWf4cXzcRZrXkZ5hd+DtViBA0QJOECUgANEmYEf7AqXZj07xt/2653H7ukKt84vOd9LZ+JLLoHlPytwgCgBB4gScIAoM/CQkWa+z6w5t64cM1+tObf2HviZFThAlIADRAk4QJSAA0QJOECUgANECThAlOvAd3bG/+twxmOCAitwgCgBB4gyQmFzboXmkffEOqzAAaIEHCBKwAGizMB3tvSrpq7GvyGFv7MCB4gScIAoAQeIMgMPeZwPn3HG+85nBGc8H7fbul9RtxWf5RzDChwgSsABogQcIMoM/GCfZ5hL54iVa6Y/7+fS7Yx63fw7+zHKMdBnBQ4QJeAAUQIOEGUGPpA9572jXD9cteb5c607r7ICB4gScIAoAQeImnaeqRngXcCaM13z4XG98xmN12qxb0+2FThAlIADRBmhsLnKLf8wMCMUgDMRcIAoAQeIcis9mzO3hm1YgQNECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANECThAlIADRAk4QJSAA0QJOECUgANE3Xfe3rTz9gBOywocIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwgSsABogQcIErAAaIEHCBKwAGiBBwg6gMrvot91uN+5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(\"tests/01.png\", 0)\n",
    "predictImage(img)\n",
    "\n",
    "img = cv2.imread(\"tests/02.png\", 0)\n",
    "predictImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
